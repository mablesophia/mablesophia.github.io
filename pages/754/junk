\chapter{Model Selection}
Suppose we observe a realization of a random variable $Y$, with
distribution defined by a parameter $\bb$
\begin{equation}
\label{gdef}
\prod_{\bx_i \in N_0} f(y_i; \bx_i,\bb) \equiv
f_\bY(\by ;\bX,\bb)  
\end{equation}
where $\by$ is the observed response associated with the
covariates $\bX$ and 
$\bb \in \mathbb{R}^P$ is a $P \times 1$ parameter vector. 

We are interested in estimating $\bb$. Suppose that before
doing so, we need to choose from amongst $P$
competing models, generated by simply restricting the
general parameter space $R^P$ in which $\bb$ lies.

In terms of the
parameters, we represent {\sl the full model} with $P$ parameters as:
\[
\mbox{Model(P): } f_\bY(\by ;\bx,\bb_P),
  \bb_P = (\beta_1,\dots,\beta_p,\beta_{p+1},\dots,\beta_P)'.
\]
We denote the ``true value'' of the parameter vector $\bb$ with
$\bb^*$. 

Akaike (1977)
formulates the problem of statistical model identification as one of
selecting a model $f_\bY(\by;\bx,\bb_p)$
based on the  
observations from that distribution, where the particular restricted
model is defined by the constraint $\beta_{p+1} = \beta_{p+2} = \dots
= \beta_{P} = 0$, so that 
\begin{equation}
\label{modelpdef}
\mbox{Model(p): } f_\bY(\by;\bx,\bb_p),
\bb_p =  (\beta_1,\dots,\beta_p,0,\dots,0)' 
\end{equation}
We will refer to $p$ as the {\sl number of parameters} and to
$\Omega_p$ as the sub-space of $\mathbb{R}^P$ defined by restriction
(\ref{modelpdef}). For each $p=1,\dots,P$, we may assume model(p) to
estimate the non-zero 
components of the vector $\bb^*$. We are interested in a 
criterion that helps us chose amongst these $P$ competing estimates.

In this Chapter we consider 3 methods for model selection.


\input{section-09-01}
\input{section-09-02}
\input{section-09-03}
\input{references-09}
\section{Mallow's $C_p$}
Mallow's  $\mbox{C}_p$  is a technique for model
selection in regression (Mallows 1973). The $\mbox{C}_p$ statistic is
defined  as a criteria 
to assess fits when models with different numbers of parameters are being
compared. It is given by
\begin{equation}
\label{Cpdef}
\mbox{C}_p = \frac{\mbox{RSS}(p)}{\sigma^2} - N + 2p
\end{equation}
If model(p) is correct then  $\mbox{C}_p$ will tend to be close to or
smaller than $p$. Therefore a simple plot of $\mbox{C}_p$ versus $p$ can be used
to decide amongst models.

In the case of ordinary linear regression, Mallow's method is based on 
estimating the mean squared error (MSE) of 
the estimator $\hat{\bg{\beta}\:}\!_p = {(\mathbf{X}_p'\mathbf{X}_p)^{-1}\mathbf{X}_p'\mathbf{Y}}$,
\[
\E [\hat{\bg{\beta}\:}\!_p - \bg{\beta}]^2
\]
via a quantity based on the residual sum of squares (RSS)
\begin{eqnarray*}
\mbox{RSS}(p) &=& \sum_{n=1}^N (y_n
-\mathbf{x}_n\hat{\bg{\beta}\:}\!_p)^2\\ 
&=& (\mathbf{Y}-\mathbf{X}_p\hat{\bg{\beta}\:}\!_p)'(\mathbf{Y}-\mathbf{X}_p\hat{\bg{\beta}\:}\!_p)\\
&=& \mathbf{Y}' ( \mathbf{I}_N - \mathbf{X}_p(\mathbf{X}_p'\mathbf{X}_p)^{-1}\mathbf{X}_p') \mathbf{Y}
\end{eqnarray*}
Here $\mathbf{I}_N$ is an $N\times N$ identity matrix.
By using a result for quadratic forms, presented for example as
Theorem 1.17 in Seber's book, page 13, namely
\[
\E[\mathbf{Y'AY}] = \E\mathbf{[Y']A}\E[\mathbf{Y}] + \tr[\bg{\Sigma}
\mathbf{A}] 
\]
$\bg{\Sigma}$ being the variance matrix of $\mathbf{Y}$,
we find that  
\begin{eqnarray*}
\E [\mbox{RSS}(p)] 
&=& \E[\mathbf{Y}' ( \mathbf{I}_N - \mathbf{X}_p(\mathbf{X}_p'\mathbf{X}_p)^{-1}\mathbf{X}_p') \mathbf{Y}]\\
&=&\E [\hat{\bg{\beta}\:}\!_p - \bg{\beta}]^2 + \tr\left[\mathbf{I}_N - \mathbf{X}_p(\mathbf{X}_p'\mathbf{X}_p)^{-1}\mathbf{X}_p'\right]\sigma^2 \\
&=& \E[ \hat{\bg{\beta}\:}\!_p - \bg{\beta}]^2 + \sigma^2\left(N - \tr
\left[{(\mathbf{X}_p'\mathbf{X}_p)(\mathbf{X}_p'\mathbf{X}_p)^{-1}}\right]\right)\\
&=& \E [\hat{\bg{\beta}\:}\!_p - \bg{\beta}]^2 + \sigma^2(N - p)
\end{eqnarray*}
where $N$ is the number of observations and $p$ is the number of
parameters. Notice that when the true model has $p$ parameters
$\E[\mbox{C}_p] = p$.  
This shows why, if model(p) is correct, $\mbox{C}_p$ will
tend to be close to $p$. 

One problem with the $\mbox{C}_p$  criterion is that
we have to find an 
appropriate estimate of $\sigma^2$ to use for all values of $p$. 

\subsection{$C_p$ for smoothers}
A more direct way of constructing an estimate of PSE is to correct the
ASR. It is easy to show that
\[
\E\{\mbox{ASR}(\lambda)\} = \left\{ 1 - n^{-1}\tr(2\bS_{\lambda} -
    \bS_{\lambda} \bS_{\lambda}') \right\} \sigma^2 + n^{-1}
    \bv_{\lambda}'\bv_{\lambda}
\]
notice that 
\[
\mbox{PSE}(\lambda) - \E\{\mbox{ASR}(\lambda)\} = n^{-1}
2\tr(\bS_{\lambda}) \sigma^2
\]

This means that if we knew $\sigma^2$ we could find a ``corrected''
ASR
\[
\mbox{ASR}(\lambda) + 2\tr(\bS_{\lambda}) \sigma^2
\]
with the right expected value. 

For linear regression $\tr(\bS_{\lambda})$ is the number of parameters
so we could think of $2\tr(\bS_{\lambda}) \sigma^2$ as a penalty
for large number of parameters or for un-smooth estimates. 

How do we obtain an estimate for $\sigma^2$? If we had a $\lambda^*$
for which the bias is 0, then the usual unbiased estimate is
\[
\frac{\sum_{i=1}^n \{y_i - f_{\lambda^*}(x_i)\}^2}{n -
  \tr(2\bS_{\lambda^*} -   \bS_{\lambda^*} \bS_{\lambda^*}')}
\]
The usual trick is to chose one a $\lambda^*$ that does little
smoothing and consider the above estimate. Another estimate that has
been proposed it the first order difference estimate
\[
\frac{1}{2(n-1)} \sum_{i=1}^{n-1} (y_{i+1} - y_i)^2
\]

Once we have an estimate $\hat{\sigma}^2$ then we can define 
\[
C_p = \mbox{ASR}(\lambda) + n^{-1} 2\tr(S_{\lambda})\hat{\sigma}^2
\]
Notice that the $p$ usually means number of parameters so it should be
$C_{\lambda}$. 


Notice this motivates a definition for degrees of freedoms.
\section{Information Criteria}
In this section we review the concepts behind 
Akaike's Information Criterion (AIC).

Akaike's original work is for
IID data, however it is extended to a regression type setting in a
straight forward way. Suppose that the conditional distribution of $Y$
given $\bx$ is know except for a $P$-dimensional parameter $\bb$. In
this case,  the probability density function of $\bY =
(Y_1,\dots,Y_n)$ can be written as 
\begin{equation}
\label{parsimodel}
f_{\bY}(\by;\bX,\bb) \equiv \prod_{i=1}^n f(y_i;\bx_i,\bb)
\end{equation}
with $\bX$ the design matrix with rows $\bx_i$. 

Assume that
there exists a true parameter vector $\bb^*$ defining a true
probability density denoted by
$f_{\bY}(\by;\bX,\bb^*)$. Given these assumptions, we wish to select
$\bb$, from one of the models 
defined as in (\ref{modelpdef}), ``nearest'' to the true
parameter $\bb^*$ based on the observed data $\by$.
The principle behind Akaike's criterion is to
define ``nearest'' as the model that minimizes the Kullback-Leibler 
Information Quantity
\begin{equation}
\label{KLdef} 
\Delta(\bb^*;\bX,\bb) = \int \left\{\log
  f_\bY(\by;\bX,\bb^*) - \log
  f_\bY(\by;\bX,\bb)\right\} f_{\bY}(\by;\bX,\bb^*) \, d\by.
\end{equation}

The analytical properties of the Kullback-Leibler Information Quantity
are discussed in detail by Kullback (1959) \nocite{kull:1959}. Two
important properties 
for Akaike's criterion are
\begin{enumerate}
\item $\Delta(\bb^*;\bX,\bb) > 0$ if 
$f_{\bY}(\by;\bX,\bb^*) \neq f_{\bY}(\by;\bX,\bb)$
\item
$\Delta(\bb^*;\bX,\bb) = 0$  if and only if $f_{\bY}(\by;\bX,\bb^*) =
f_{\bY}(\by;\bX,\bb)$ 
\end{enumerate}
almost everywhere on the range of $\bY$. 
The properties
mentioned suggest that finding the model that 
minimizes the Kullback-Leibler 
Information Quantity is an appropriate way to choose the ``nearest''
model.

Since the first term on the
right hand side of 
(\ref{KLdef}) is constant over all models we consider, we may instead
maximize
\begin{eqnarray}
\nonumber
H(\bb) &=& \int \log
  f_\bY(\by;\bX,\bb) f_{\bY}(\by;\bX,\bb^*) \, d\by\\
\label{Hdef}
&=& \sum_{i=1}^n \int \log f(y_i;\bX,\bb) \, f(y_i;\bx_i,\bb^*) \, dy_i.
\end{eqnarray}

Let $\bbh_p$ be the maximum likelihood estimate under
Model(p). Akaike's procedure for model selection is based on choosing
the model which produces the estimate that maximizes 
$\E_{\bb^*}\left[H(\bbh_p)\right]$ amongst all
competing models. Akaike then derives a criterion by constructing an
asymptotically unbiased estimate of $\E_{\bb^*}\left[H(\bbh_p)\right]$ based
on the observed data. 

Notice that $H(\bbh_p)$ is a function,
defined by (\ref{Hdef}),
of the maximum likelihood estimate $\bbh_p$, which is a
random variable obtained from the observed data.
A natural estimator of 
its expected value (under the true distribution of the data) is
obtained by substituting the empirical distribution of the data into
(\ref{Hdef}) 
resulting in the log likelihood equation evaluated at the maximum
likelihood estimate under model(p)
\[
l(\bbh_p) = \sum_{i=1}^n \log f(y_i;\bx_i,\bbh_p).
\]
Akaike noticed that in general  $l(\bbh_p)$ will 
overestimate $\E_{\bb^*}\left[H(\bbh)\right]$.  In particular Akaike found
that under some regularity conditions
\[
E_{\bb^*}\left[l(\bbh_p) - H(\bbh_p)\right] \approx p .
\]
This suggests that larger 
values of $p$ will result in smaller values 
of $l(\bbh_p)$, which may be
incorrectly interpreted as a ``better'' 
fit, regardless of the true model. We need to ``penalize'' for larger
values of $p$ in order to obtain an unbiased estimate of
the ``closeness'' of the model. 
This fact leads to the Akaike Information Criteria which is a
bias-corrected estimate given by 
\begin{equation}
\label{AIC}
\mbox{AIC}(p) = - 2 l(\bbh_{p}) + 2p .
\end{equation}
See, for example, Akaike (1973) and Bozdogan (1987)  for
the details.  


\section{Posterior Probability Criteria}
Objections have been raised that minimizing Akaike's criterion does
not produce asymptotically consistent estimates of the correct model
Notice that if 
we consider Model($p^*$) as the correct model then we have
for any $p>p*$ 
\begin{equation}
\label{prob}
\Pr\left[ AIC(p) < AIC(p^*) \right] = \Pr\left[ 2 \{l(\bbh_{p}) - l(\bbh_{p^*})
\} > 2(p - p^*)\right].
\end{equation}
Notice that, in this case, the random variable $2\{l(\bbh_{p}) -
l(\bbh_{p^*})\}$ is  
the logarithm of the likelihood ratio of two competing models which,
under certain regularity 
conditions, is known to converge in 
distribution to $\chi^2_{p-p^*}$, and thus it follows that the
probability in Equation (\ref{prob}) is not 0
asymptotically.  
Some have suggested multiplying
the penalty term in the 
AIC by some increasing function of $n$, say $a(n)$, that makes the
probability
\[
 \Pr\left[ 2 \{l(\bbh_{p}) - l(\bbh_{p^*})
\} > 2a(n)(p - p^*)\right] 
\]
asymptotically equal to 0. There are many choices of $a(n)$ that would
work in this context. However, some of the choices made in the
literature seem arbitrary.

Schwarz (1978) \nocite{schw:1978} and Kashyap (1982) \nocite{kash:1982} suggest using a
Bayesian approach 
to the problem of model selection which, in the IID case,  results in a 
criterion that is similar to AIC in that it is based on a penalized
log-likelihood function evaluated at the maximum likelihood estimate
for the model in question. The
penalty term in the Bayesian Information Criteria (BIC) obtained by
Schwarz (1978) \nocite{schw:1978} 
is the AIC penalty term $p$ multiplied by the function $a(n)
=\frac{1}{2}\log(N)$.   

The Bayesian approach to model selection is based on maximizing the
posterior probabilities of the alternative models, given the
observations. To do this we must define a strictly positive prior
probability 
$\pi_p = \Pr[\mbox{Model}(p)] $ for each model and a conditional prior
$d\mu_p(\bb)$ for the parameter given it is in $\Omega_p$, the subspace
defined by Model(p). Let $\bY = (Y_1,\dots,Y_n)$ be the
response variable and define the distribution given $\bb$
following (\ref{parsimodel}) 
\[
f_{\bY}(\by| \bX,\bb) \equiv \prod_{i=1}^n f(y_i;\bx_i,\bb)
\]
The posterior probability that we look to
maximize is  
\[
\Pr\left[\mbox{Model}(p) | \bY=\by \right] = 
\frac{\int_{\Omega_p} \pi_p f_{\bY}(\by|\bX,\bb) d\mu_p(\bb)}
{\sum_{q=1}^P \int_{\Omega_q} \pi_q f_{\bY}(\by|\bX,\bb) d\mu_q(\bb)}
\]
Notice that the denominator depends neither on the model nor the data,
so we need only to maximize the numerator when choosing models.

Schwarz (1978) \nocite{schw:1978} and Kashyap (1982)
\nocite{kash:1982} suggest criteria derived 
by taking a Taylor expansion of the log posterior probabilities of the 
alternative models. Schwarz (1978) \nocite{schw:1978} presents the following
approximation for the IID case
\[
\log \int_{\Omega_p} \pi_p f_{\bY}(\by|\bX,\bb) d\mu_p(\bb)
\approx  l(\bbh_{p}) - \frac{1}{2} p
\log n 
\] 
with $\bbh_p$ the maximum likelihood estimate obtained under Model(p).

This fact leads to the Bayesian Information Criteria (BIC) which is
\begin{equation}
\label{BIC}
\mbox{BIC}(p) = - 2 l(\bbh_{p}) + p\log n
\end{equation}


\subsection{Kyphosis Example}
The AIC
and BIC obtained for the gam are: 


\begin{verbatim}
AIC(Age) =  83              BIC(Age) = 90
AIC(Age,Start) = 64         BIC(Age,Start) = 78
AIC(Age,Number) = 73        BIC(Age,Number) = 86
AIC(Age,Start,Number) = 60  BIC(Age,Start,Number) = 81
\end{verbatim}  
\begin{thebibliography}{}

\bibitem{1}
{Akaike, H.} (1973),
 ``Information theory and an extension of the maximum likelihood
  principle,''
 in {Petrov, B. and Csaki, B.}, editors, {\it Second International
  Symposium on Information Theory}, pp. 267--281, Budapest: Academiai
 Kiado. 

\bibitem{2}
{Bozdogan, H.} (1987),
 ``Model selection and {A}kaike's information criterion ({AIC}): The
  general theory and its analytical extensions,''
 {\it Psychometrica}, 52, 345--370.


\bibitem{3} {Bozdogan, H.} (1994),
 ``Mixture-model cluster analysis using a new informational complexity
  and model selection criteria,''
 in {Bozdogan, H.}, editor, {\it Multivariate Statistical Modeling},
  volume~2, pp. 69--113, The Netherlands: Dordrecht.

\bibitem{4}
{Kullback, S.} (1959),
 {\it Information Theory and Statistics},
 New York: John Wiley \& Sons.


\bibitem{5}
{Schwarz, G.} (1978),
 ``Estimating the dimension of a model,''
 {\it Annals of Statistics}, 6, 461--464.



\bibitem{6}
{Shibata, R.} (1989),
 ``Statistical aspects of model selection,''
 in {Williems, J.~C.}, editor, {\it From Data to Model}, pp.
  215--240, New York: Springer-Verlag.


\end{thebibliography}
